package neuralnetwork

//Activation will handle the different activation types

//Sigmoid processes based on a sigmoid function
func Sigmoid(a float64) float64 {
	return a
}

//Relu uses a relu function to determine activation
func Relu(a float64) float64 {
	return a
}

//Linear uses a linear activation function
func Linear(a float64) float64 {
	return a
}

//Binary step uses a binary activation function
func BinaryStep(a float64) float64 {
	return a
}

//Tahn
func Tahn(a float64) float64 {
	return a
}

//LeakyRelu
func LeakyRelu(a float64) float64 {
	return a
}

//Swish
func Swish(a float64) float64 {
	return a
}

//Elu
func Elu(a float64) float64 {
	return a
}

//Gelu
func Gelu(a float64) float64 {
	return a
}

//Selu
func Selu(a float64) float64 {
	return a
}
